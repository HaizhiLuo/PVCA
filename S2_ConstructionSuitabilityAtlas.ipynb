{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda0d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: 读取 TIF 并报告 nodata 与有效数值范围\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.errors import RasterioIOError\n",
    "from tqdm import tqdm\n",
    "\n",
    "def report_tifs(folder=\"Factor_aligned\", pattern=\"*.tif\"):\n",
    "    tif_paths = sorted(glob.glob(os.path.join(folder, pattern)))\n",
    "    if not tif_paths:\n",
    "        raise FileNotFoundError(f\"No tif files found in folder: {folder}\")\n",
    "\n",
    "    print(f\"Found {len(tif_paths)} files in '{folder}'.\\n\")\n",
    "\n",
    "    for p in tqdm(tif_paths, desc=\"Processing files\", unit=\"file\"):\n",
    "        try:\n",
    "            with rasterio.open(p) as src:\n",
    "                # 读取第一波段（若为多波段，根据需要修改）\n",
    "                data = src.read(1, masked=True)  # 返回 numpy.ma.MaskedArray（若存在 nodata 则自动掩膜）\n",
    "                nodata_from_meta = src.nodata\n",
    "                dtype = src.dtypes[0] if src.count >= 1 else src.dtypes\n",
    "                total_pixels = data.size\n",
    "\n",
    "                # 确定 nodata 情况\n",
    "                # rasterio 在 read(..., masked=True) 时，会把 nodata 与有效掩膜合并为 mask\n",
    "                if isinstance(data, np.ma.MaskedArray):\n",
    "                    nodata_mask = data.mask  # True 表示被掩膜（nodata 或 invalid）\n",
    "                    nodata_count = int(nodata_mask.sum())\n",
    "                    valid_arr = data.compressed()  # 返回去除掩膜后的 1D 有效值\n",
    "                else:\n",
    "                    # 若不是 MaskedArray，则检查 NaN\n",
    "                    nan_mask = np.isnan(data)\n",
    "                    nodata_count = int(nan_mask.sum())\n",
    "                    valid_arr = data[~nan_mask].ravel()\n",
    "\n",
    "                # 当 metadata 没有 nodata 时，提示并继续\n",
    "                if nodata_from_meta is None:\n",
    "                    meta_note = \"nodata (metadata): None\"\n",
    "                    # 若没有掩膜且没有 NaN，说明文件元数据/像元均无显式 nodata\n",
    "                    if nodata_count == 0:\n",
    "                        inferred_note = \"inferred nodata: none detected\"\n",
    "                    else:\n",
    "                        inferred_note = f\"inferred nodata pixels (from mask/NaN): {nodata_count}\"\n",
    "                else:\n",
    "                    meta_note = f\"nodata (metadata): {nodata_from_meta}\"\n",
    "                    inferred_note = f\"masked/NaN pixels: {nodata_count}\"\n",
    "\n",
    "                # 计算有效值范围\n",
    "                if valid_arr.size > 0:\n",
    "                    vmin = float(np.nanmin(valid_arr))\n",
    "                    vmax = float(np.nanmax(valid_arr))\n",
    "                    mean = float(np.nanmean(valid_arr))\n",
    "                    median = float(np.nanmedian(valid_arr))\n",
    "                    valid_count = valid_arr.size\n",
    "                    valid_pct = valid_count / total_pixels * 100.0\n",
    "                else:\n",
    "                    vmin = vmax = mean = median = None\n",
    "                    valid_count = 0\n",
    "                    valid_pct = 0.0\n",
    "\n",
    "                # 输出摘要\n",
    "                print(f\"\\nFile: {os.path.basename(p)}\")\n",
    "                print(f\"  dtype: {dtype}, shape: {src.height}x{src.width} ({total_pixels} px)\")\n",
    "                print(f\"  {meta_note}; {inferred_note}.\")\n",
    "                print(f\"  Valid pixels: {valid_count} ({valid_pct:.2f}%)\")\n",
    "                if vmin is not None:\n",
    "                    print(f\"  Valid value range: min = {vmin}, max = {vmax}\")\n",
    "                    print(f\"  Stats (valid): mean = {mean:.4f}, median = {median:.4f}\")\n",
    "                else:\n",
    "                    print(\"  No valid pixels found (all masked or nodata).\")\n",
    "\n",
    "        except RasterioIOError as e:\n",
    "            print(f\"\\nFile: {os.path.basename(p)} - ERROR opening file: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nFile: {os.path.basename(p)} - Unexpected error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 若需在不同路径运行，请修改 folder 参数\n",
    "    report_tifs(folder=\"Factor_aligned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75a171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "input_dir = \"Factor_aligned\"\n",
    "output_dir = \"Scaled_1_100\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "NODATA = -32768\n",
    "\n",
    "# 模糊匹配关键词：文件名中出现这些关键词即可触发方法\n",
    "method_map = {\n",
    "    \"E1_DEM\": (\"log_p2p98\", [\"DEM\"]),\n",
    "    \"E2_Slope\": (\"p2p98\", [\"Slope\"]),\n",
    "    \"E3_Orientation\": (\"minmax\", [\"Orientation\"]),\n",
    "    \"E4_NDVI\": (\"minmax\", [\"NDVI\"]),\n",
    "    \"E5_RHU\": (\"p2p98\", [\"RHU\"]),\n",
    "    \"E6_SSD\": (\"p2p98\", [\"SSD\"]),\n",
    "    \"E8_WIN\": (\"p2p98\", [\"WIN\"]),\n",
    "    \"E10_LST\": (\"minmax\", [\"LST\"]),\n",
    "\n",
    "    \"S1_PublicService\": (\"log_p2p98\", [\"PublicService\"]),\n",
    "    \"S2_DistanceFromUrbanArea\": (\"log_p2p98\", [\"Distance\", \"UrbanArea\"]),\n",
    "    \"S4_Supply\": (\"p2p98\", [\"Supply\"]),\n",
    "    \"S9_NighttimeLights\": (\"log_p2p98\", [\"Light\", \"Night\", \"VIIRS\"]),\n",
    "\n",
    "    \"LULC_2020\": (\"skip\", [\"LULC\"])\n",
    "}\n",
    "\n",
    "# 查找 factor key\n",
    "def find_factor_key(filename):\n",
    "    fname = filename.lower()\n",
    "    for key, (method, keywords) in method_map.items():\n",
    "        for kw in keywords:\n",
    "            if kw.lower() in fname:\n",
    "                return key, method\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def scale_minmax(arr, vmin, vmax):\n",
    "    return (arr - vmin) / (vmax - vmin) * 99 + 1\n",
    "\n",
    "\n",
    "def robust_p2p98(arr):\n",
    "    return np.nanpercentile(arr, 2), np.nanpercentile(arr, 98)\n",
    "\n",
    "\n",
    "def log1p_safe(arr):\n",
    "    arr2 = arr.copy()\n",
    "    arr2[arr2 < 0] = 0\n",
    "    return np.log1p(arr2)\n",
    "\n",
    "\n",
    "records = []\n",
    "\n",
    "for fname in os.listdir(input_dir):\n",
    "\n",
    "    if not fname.endswith(\".tif\"):\n",
    "        continue\n",
    "\n",
    "    key, method = find_factor_key(fname)\n",
    "    if key is None:\n",
    "        print(f\"[Skip] {fname}: method not defined by fuzzy match.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n---- Processing {fname} | Key={key} | Method={method} ----\")\n",
    "\n",
    "    # 分类数据跳过\n",
    "    if method == \"skip\":\n",
    "        print(\"Classification raster, skipping scaling.\")\n",
    "        continue\n",
    "\n",
    "    fpath = os.path.join(input_dir, fname)\n",
    "\n",
    "    # 读取数据\n",
    "    with rasterio.open(fpath) as src:\n",
    "        arr = src.read(1).astype(float)\n",
    "        mask = (arr == src.nodata) if src.nodata is not None else np.isnan(arr)\n",
    "        arr[mask] = np.nan\n",
    "\n",
    "    # 应用不同缩放方式\n",
    "    if method == \"minmax\":\n",
    "        vmin, vmax = np.nanmin(arr), np.nanmax(arr)\n",
    "        scaled = scale_minmax(arr, vmin, vmax)\n",
    "\n",
    "    elif method == \"p2p98\":\n",
    "        p2, p98 = robust_p2p98(arr)\n",
    "        clipped = np.clip(arr, p2, p98)\n",
    "        scaled = scale_minmax(clipped, p2, p98)\n",
    "        vmin, vmax = p2, p98\n",
    "\n",
    "    elif method == \"log_p2p98\":\n",
    "        arr2 = log1p_safe(arr)\n",
    "        p2, p98 = robust_p2p98(arr2)\n",
    "        clipped = np.clip(arr2, p2, p98)\n",
    "        scaled = scale_minmax(clipped, p2, p98)\n",
    "        vmin, vmax = p2, p98\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "    # 清理 NaN，转 int16\n",
    "    scaled[np.isnan(scaled)] = np.nan\n",
    "    scaled_int = scaled.astype(np.float32)   # 先浮点避免溢出\n",
    "    scaled_int[np.isnan(scaled_int)] = NODATA\n",
    "    scaled_int = scaled_int.astype(np.int16)\n",
    "\n",
    "    outpath = os.path.join(output_dir, fname)\n",
    "\n",
    "    # 写出文件\n",
    "    with rasterio.open(fpath) as src:\n",
    "        profile = src.profile\n",
    "        profile.update(dtype=rasterio.int16, nodata=NODATA)\n",
    "\n",
    "        with rasterio.open(outpath, \"w\", **profile) as dst:\n",
    "            dst.write(scaled_int, 1)\n",
    "\n",
    "    # 记录映射信息\n",
    "    records.append({\n",
    "        \"factor_key\": key,\n",
    "        \"file\": fname,\n",
    "        \"method\": method,\n",
    "        \"value_min_used\": float(vmin),\n",
    "        \"value_max_used\": float(vmax),\n",
    "        \"output_file\": outpath,\n",
    "        \"nodata\": NODATA\n",
    "    })\n",
    "\n",
    "# 保存映射表\n",
    "df = pd.DataFrame(records)\n",
    "df.to_csv(\"scaling_mapping_table.csv\", index=False)\n",
    "\n",
    "print(\"\\nAll done. Mapping table saved: scaling_mapping_table.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bdbba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 输入文件（Excel）与映射表（CSV）\n",
    "mapping_csv = \"scaling_mapping_table.csv\"  # 之前输出的映射表\n",
    "excel_files = [\n",
    "    \"Expansion_RF_shap_values.xlsx\",\n",
    "    \"Shrink_RF_shap_values.xlsx\"\n",
    "]\n",
    "\n",
    "# 若映射表使用不同名称，请修改上述 mapping_csv\n",
    "if not os.path.exists(mapping_csv):\n",
    "    raise FileNotFoundError(f\"Mapping file not found: {mapping_csv}\")\n",
    "\n",
    "# 读取映射表：期望列包含 factor_key, method, value_min_used, value_max_used\n",
    "mapping_df = pd.read_csv(mapping_csv, dtype={\"factor_key\": str})\n",
    "# 确保列名兼容（兼容不同导出名）\n",
    "# 支持的列名： factor_key / factor / method / min_used / max_used / value_min_used / value_max_used\n",
    "def _col(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "col_key = _col(mapping_df, [\"factor_key\", \"factor\", \"Factor\"])\n",
    "col_method = _col(mapping_df, [\"method\", \"Method\"])\n",
    "col_min = _col(mapping_df, [\"value_min_used\", \"min_used\", \"orig_min\", \"min_used\"])\n",
    "col_max = _col(mapping_df, [\"value_max_used\", \"max_used\", \"orig_max\", \"max_used\"])\n",
    "\n",
    "if not all([col_key, col_method, col_min, col_max]):\n",
    "    raise ValueError(\"mapping CSV missing required columns. Found columns: \" + \", \".join(mapping_df.columns))\n",
    "\n",
    "# 建立查找字典：key -> {method, vmin, vmax}\n",
    "mapping = {}\n",
    "for _, row in mapping_df.iterrows():\n",
    "    key = str(row[col_key]).strip()\n",
    "    method = str(row[col_method]).strip()\n",
    "    try:\n",
    "        vmin = float(row[col_min])\n",
    "        vmax = float(row[col_max])\n",
    "    except Exception:\n",
    "        vmin, vmax = np.nan, np.nan\n",
    "    mapping[key] = {\"method\": method, \"vmin\": vmin, \"vmax\": vmax}\n",
    "\n",
    "# 模糊匹配：根据列名中包含的关键字匹配到 mapping 的 key\n",
    "def find_mapping_for_col(colname):\n",
    "    name = colname.lower()\n",
    "    # 直接按常见 token 顺序匹配\n",
    "    for key in mapping.keys():\n",
    "        k = key.lower()\n",
    "        if k in name:\n",
    "            return key, mapping[key]\n",
    "    # 其次按部分 token（拆 key）\n",
    "    for key in mapping.keys():\n",
    "        kparts = [p for p in key.lower().split(\"_\") if p]\n",
    "        for p in kparts:\n",
    "            if p and p in name:\n",
    "                return key, mapping[key]\n",
    "    return None, None\n",
    "\n",
    "# 缩放函数：输入原始值和映射参数，输出 1-100 的整数或 NaN\n",
    "def map_value_to_1_100(x, method, vmin, vmax):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    try:\n",
    "        xv = float(x)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "    # skip / categorical\n",
    "    if method.lower() in (\"skip\", \"categorical\"):\n",
    "        return xv  # 不改变分类值（LULC）\n",
    "    # log_p2p98\n",
    "    if method.lower() in (\"log_p2p98\", \"log-p2p98\", \"log_p2-p98\", \"log1p_p2p98\", \"log1p->p2p98\"):\n",
    "        # 先处理负值：与栅格一致，负值设为 0\n",
    "        xv2 = max(xv, 0.0)\n",
    "        xv2 = math.log1p(xv2)\n",
    "        if math.isnan(vmin) or math.isnan(vmax) or vmax == vmin:\n",
    "            return np.nan\n",
    "        # 裁剪后线性到 1-100\n",
    "        xv2 = min(max(xv2, vmin), vmax)\n",
    "        scaled = (xv2 - vmin) / (vmax - vmin) * 99.0 + 1.0\n",
    "        # 截断到 [1,100] 并取整\n",
    "        scaled = min(max(scaled, 1.0), 100.0)\n",
    "        return int(round(scaled))\n",
    "\n",
    "    # p2p98 (已在 mapping 表中给出 vmin/vmax 为 p2/p98)\n",
    "    if method.lower() in (\"p2p98\", \"p2-p98\", \"p2_p98\"):\n",
    "        if math.isnan(vmin) or math.isnan(vmax) or vmax == vmin:\n",
    "            return np.nan\n",
    "        xv2 = min(max(xv, vmin), vmax)\n",
    "        scaled = (xv2 - vmin) / (vmax - vmin) * 99.0 + 1.0\n",
    "        scaled = min(max(scaled, 1.0), 100.0)\n",
    "        return int(round(scaled))\n",
    "\n",
    "    # plain minmax\n",
    "    if method.lower() in (\"minmax\", \"min_max\", \"min-max\"):\n",
    "        if math.isnan(vmin) or math.isnan(vmax) or vmax == vmin:\n",
    "            return np.nan\n",
    "        xv2 = min(max(xv, vmin), vmax)\n",
    "        scaled = (xv2 - vmin) / (vmax - vmin) * 99.0 + 1.0\n",
    "        scaled = min(max(scaled, 1.0), 100.0)\n",
    "        return int(round(scaled))\n",
    "\n",
    "    # fallback: try minmax\n",
    "    if math.isnan(vmin) or math.isnan(vmax) or vmax == vmin:\n",
    "        return np.nan\n",
    "    xv2 = min(max(xv, vmin), vmax)\n",
    "    scaled = (xv2 - vmin) / (vmax - vmin) * 99.0 + 1.0\n",
    "    scaled = min(max(scaled, 1.0), 100.0)\n",
    "    return int(round(scaled))\n",
    "\n",
    "# 主循环：处理每个 Excel，按列映射 Feature_* 到 Feature_*_mapped\n",
    "for excel in excel_files:\n",
    "    if not os.path.exists(excel):\n",
    "        print(f\"[Warning] file not found: {excel}; skip.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nProcessing Excel: {excel}\")\n",
    "    df = pd.read_excel(excel)\n",
    "\n",
    "    out_df = df.copy()\n",
    "\n",
    "    # 查找所有以 Feature_ 开头的列\n",
    "    feature_cols = [c for c in df.columns if str(c).startswith(\"Feature_\")]\n",
    "\n",
    "    if not feature_cols:\n",
    "        print(\"  No Feature_ columns found. Skipping file.\")\n",
    "        continue\n",
    "\n",
    "    for col in feature_cols:\n",
    "        key, mapinfo = find_mapping_for_col(col)\n",
    "        if key is None:\n",
    "            print(f\"  [Skip] Feature column '{col}' has no mapping key found. Leave unchanged.\")\n",
    "            continue\n",
    "\n",
    "        method = mapinfo[\"method\"]\n",
    "        vmin = mapinfo[\"vmin\"]\n",
    "        vmax = mapinfo[\"vmax\"]\n",
    "\n",
    "        # 生成新列名\n",
    "        newcol = f\"{col}_mapped\"\n",
    "        # 应用映射（逐元素）\n",
    "        out_df[newcol] = df[col].apply(lambda z: map_value_to_1_100(z, method, vmin, vmax))\n",
    "\n",
    "        print(f\"  Mapped {col} -> {newcol} using key='{key}', method='{method}', vmin={vmin}, vmax={vmax}\")\n",
    "\n",
    "    # 输出文件\n",
    "    outname = os.path.splitext(excel)[0] + \"_mapped.xlsx\"\n",
    "    out_df.to_excel(outname, index=False)\n",
    "    print(f\"Saved mapped Excel: {outname}\")\n",
    "\n",
    "print(\"\\nAll Excel files processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c5b424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 输入 Excel 文件列表\n",
    "excel_files = [\n",
    "    \"Expansion_RF_shap_values.xlsx\",\n",
    "    \"Shrink_RF_shap_values.xlsx\"\n",
    "]\n",
    "\n",
    "for excel in excel_files:\n",
    "    if not os.path.exists(excel):\n",
    "        print(f\"[Warning] file not found: {excel}; skip.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nProcessing SHAP values in: {excel}\")\n",
    "    df = pd.read_excel(excel)\n",
    "\n",
    "    # 查找所有 SHAP_ 开头的列\n",
    "    shap_cols = [c for c in df.columns if str(c).startswith(\"SHAP_\")]\n",
    "    if not shap_cols:\n",
    "        print(\"  No SHAP_ columns found. Skipping file.\")\n",
    "        continue\n",
    "\n",
    "    # 统计 SHAP 值分布\n",
    "    all_shap_values = df[shap_cols].values.flatten()\n",
    "    all_shap_values = all_shap_values[~np.isnan(all_shap_values)]  # 去掉 NaN\n",
    "\n",
    "    print(\"  SHAP value distribution (before processing):\")\n",
    "    print(f\"    Count: {len(all_shap_values)}\")\n",
    "    print(f\"    Min: {np.min(all_shap_values):.6f}\")\n",
    "    print(f\"    Max: {np.max(all_shap_values):.6f}\")\n",
    "    print(f\"    Mean: {np.mean(all_shap_values):.6f}\")\n",
    "    print(f\"    Median: {np.median(all_shap_values):.6f}\")\n",
    "    print(f\"    25%: {np.percentile(all_shap_values, 25):.6f}\")\n",
    "    print(f\"    75%: {np.percentile(all_shap_values, 75):.6f}\")\n",
    "\n",
    "    # 处理 SHAP 值：负值置为 0\n",
    "    for col in shap_cols:\n",
    "        newcol = f\"{col}_processed\"\n",
    "        df[newcol] = df[col].apply(lambda x: max(x, 0) if not pd.isna(x) else np.nan)\n",
    "\n",
    "    # 保存新 Excel\n",
    "    outname = os.path.splitext(excel)[0] + \"_shap_processed.xlsx\"\n",
    "    df.to_excel(outname, index=False)\n",
    "    print(f\"  Saved processed SHAP values to: {outname}\")\n",
    "\n",
    "print(\"\\nAll SHAP values processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bef187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 输入 Excel 文件列表\n",
    "excel_files = [\n",
    "    \"Expansion_RF_shap_values.xlsx\",\n",
    "    \"Shrink_RF_shap_values.xlsx\"\n",
    "]\n",
    "\n",
    "# 特征映射 1–100（百分位线性映射，去掉极值2-98%）\n",
    "def feature_map_1_100(series):\n",
    "    # 去掉 NaN\n",
    "    s = series.dropna()\n",
    "    p2, p98 = np.percentile(s, [2, 98])\n",
    "    s_clipped = series.clip(lower=p2, upper=p98)\n",
    "    # 线性映射 1-100\n",
    "    scaled = ((s_clipped - p2) / (p98 - p2) * 99 + 1).round()\n",
    "    return scaled.astype(int)\n",
    "\n",
    "# SHAP 映射 0–255\n",
    "def shap_map_0_255(series):\n",
    "    series_proc = series.copy()\n",
    "    series_proc[series_proc < 0] = 0\n",
    "    pos = series_proc[series_proc > 0]\n",
    "    if len(pos) == 0:\n",
    "        return pd.Series([0]*len(series_proc), index=series_proc.index)\n",
    "    ranks = pos.rank(method='min')\n",
    "    scaled = ((ranks - 1) / (len(pos) - 1) * 254 + 1).round()\n",
    "    series_mapped = pd.Series(0, index=series_proc.index)\n",
    "    series_mapped.loc[pos.index] = scaled.astype(int)\n",
    "    return series_mapped\n",
    "\n",
    "for excel in excel_files:\n",
    "    if not os.path.exists(excel):\n",
    "        print(f\"[Warning] file not found: {excel}; skip.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nProcessing file: {excel}\")\n",
    "    df = pd.read_excel(excel)\n",
    "\n",
    "    # 找到 Feature_ 和 SHAP_ 列\n",
    "    feature_cols = [c for c in df.columns if str(c).startswith(\"Feature_\")]\n",
    "    shap_cols = [c for c in df.columns if str(c).startswith(\"SHAP_\")]\n",
    "\n",
    "    # 生成 Feature 1–100 列\n",
    "    for f in feature_cols:\n",
    "        score_col = f\"{f}_score\"\n",
    "        df[score_col] = feature_map_1_100(df[f])\n",
    "        print(f\"  Feature mapped: {f} -> {score_col}\")\n",
    "\n",
    "    # 生成 SHAP 0–255 列\n",
    "    for s in shap_cols:\n",
    "        score_col = f\"{s}_score\"\n",
    "        df[score_col] = shap_map_0_255(df[s])\n",
    "        print(f\"  SHAP mapped: {s} -> {score_col}\")\n",
    "\n",
    "    # 输出 Excel：只保留原始 Feature、Feature 1–100、SHAP 0–255\n",
    "    selected_cols = []\n",
    "    for f in feature_cols:\n",
    "        selected_cols.append(f)\n",
    "        selected_cols.append(f\"{f}_score\")\n",
    "    selected_cols += [f\"{s}_score\" for s in shap_cols]\n",
    "\n",
    "    df_out = df[selected_cols]\n",
    "    outname = os.path.splitext(excel)[0] + \"_processed.xlsx\"\n",
    "    df_out.to_excel(outname, index=False)\n",
    "    print(f\"  Saved processed file to: {outname}\")\n",
    "\n",
    "print(\"\\nAll files processed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7377cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# 文件列表\n",
    "excel_files = {\n",
    "    \"Expansion\": \"SHAP赋分结果_Expan.xlsx\",\n",
    "    \"Shrink\": \"SHAP赋分结果_Shrink.xlsx\"\n",
    "}\n",
    "\n",
    "for mode, excel in excel_files.items():\n",
    "    if not os.path.exists(excel):\n",
    "        print(f\"[Warning] file not found: {excel}; skip {mode}.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nProcessing {mode} SHAP mapping table from: {excel}\")\n",
    "    df = pd.read_excel(excel)\n",
    "\n",
    "    # 去除列名空格\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    # Feature 和 SHAP 列\n",
    "    feature_cols = [c for c in df.columns if c.startswith(\"Feature_\") and not c.endswith(\"_score\") == False]\n",
    "    feature_score_cols = [c for c in df.columns if c.startswith(\"Feature_\") and c.endswith(\"_score\")]\n",
    "    shap_score_cols = [c for c in df.columns if c.startswith(\"SHAP_\") and c.endswith(\"_score\")]\n",
    "\n",
    "    mapping_records = []\n",
    "\n",
    "    for f_feat, f_feat_score, f_shap_score in zip(feature_cols, feature_score_cols, shap_score_cols):\n",
    "        # 特殊处理土地利用\n",
    "        if f_feat == \"Feature_LULC_2020\":\n",
    "            scores = np.arange(1, 14)\n",
    "            shap_values = []\n",
    "            remarks = []\n",
    "            for s in scores:\n",
    "                mask = df[f_feat] == s\n",
    "                if mask.sum() > 0:\n",
    "                    shap_values.append(round(df.loc[mask, f_shap_score].mean(),2))\n",
    "                    remarks.append(\"\")\n",
    "                else:\n",
    "                    shap_values.append(np.nan)  # 用插值后填充\n",
    "                    remarks.append(\"插值\")\n",
    "            # 插值\n",
    "            nan_idx = [i for i,v in enumerate(shap_values) if np.isnan(v)]\n",
    "            if nan_idx:\n",
    "                x = np.array([i for i,v in enumerate(shap_values) if not np.isnan(v)])\n",
    "                y = np.array([v for v in shap_values if not np.isnan(v)])\n",
    "                f_interp = interp1d(x, y, kind='linear', fill_value='extrapolate')\n",
    "                for idx in nan_idx:\n",
    "                    shap_values[idx] = round(float(f_interp(idx)),2)\n",
    "        else:\n",
    "            # 正常特征 1-100\n",
    "            scores = np.arange(1,101)\n",
    "            temp = df[[f_feat_score, f_shap_score]].copy()\n",
    "            group = temp.groupby(f_feat_score, as_index=False).agg({f_shap_score:'mean'})\n",
    "            # 构建插值函数\n",
    "            interp_func = interp1d(group[f_feat_score], group[f_shap_score], kind='linear', bounds_error=False, fill_value='extrapolate')\n",
    "            shap_values = [round(float(interp_func(s)),2) for s in scores]\n",
    "            # 备注\n",
    "            existing_scores = set(group[f_feat_score].values)\n",
    "            remarks = [\"插值\" if s not in existing_scores else \"\" for s in scores]\n",
    "\n",
    "        # 保存记录\n",
    "        for s, v, r in zip(scores, shap_values, remarks):\n",
    "            mapping_records.append({\n",
    "                \"Feature\": f_feat,\n",
    "                \"Score\": int(s),\n",
    "                \"SHAP_mean\": v,\n",
    "                \"Remark\": r\n",
    "            })\n",
    "\n",
    "    # 保存 Excel\n",
    "    df_mapping = pd.DataFrame(mapping_records)\n",
    "    outname = f\"{mode}_FeatureScore_SHAP_mapping.xlsx\"\n",
    "    df_mapping.to_excel(outname, index=False)\n",
    "    print(f\"  Saved {mode} mapping table to: {outname}\")\n",
    "\n",
    "print(\"\\nAll mapping tables generated successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618bf051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "import os\n",
    "\n",
    "# 文件名\n",
    "excel_files = {\n",
    "    \"Expansion\": \"SHAP赋分结果_Expan.xlsx\",\n",
    "    \"Shrink\": \"SHAP赋分结果_Shrink.xlsx\"\n",
    "}\n",
    "\n",
    "for mode, excel in excel_files.items():\n",
    "    if not os.path.exists(excel):\n",
    "        print(f\"[Warning] file not found: {excel}; skip {mode}.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nProcessing LULC SHAP mapping for: {mode}\")\n",
    "    df = pd.read_excel(excel)\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    f_feat = \"Feature_LULC_2020\"\n",
    "    f_shap_score = \"SHAP_LULC_2020_score\"\n",
    "\n",
    "    # 1-13 类\n",
    "    scores = np.arange(1,14)\n",
    "    shap_values = []\n",
    "    remarks = []\n",
    "\n",
    "    for s in scores:\n",
    "        mask = df[f_feat] == s\n",
    "        if mask.sum() > 0:\n",
    "            shap_values.append(round(df.loc[mask, f_shap_score].mean(),2))\n",
    "            remarks.append(\"\")\n",
    "        else:\n",
    "            shap_values.append(np.nan)\n",
    "            remarks.append(\"插值\")\n",
    "\n",
    "    # 插值处理\n",
    "    nan_idx = [i for i,v in enumerate(shap_values) if np.isnan(v)]\n",
    "    if nan_idx:\n",
    "        x = np.array([i for i,v in enumerate(shap_values) if not np.isnan(v)])\n",
    "        y = np.array([v for v in shap_values if not np.isnan(v)])\n",
    "        f_interp = interp1d(x, y, kind='linear', fill_value='extrapolate')\n",
    "        for idx in nan_idx:\n",
    "            shap_values[idx] = round(float(f_interp(idx)),2)\n",
    "\n",
    "    # 保存记录\n",
    "    mapping_records = []\n",
    "    for s, v, r in zip(scores, shap_values, remarks):\n",
    "        mapping_records.append({\n",
    "            \"Feature\": f_feat,\n",
    "            \"Score\": int(s),\n",
    "            \"SHAP_mean\": v,\n",
    "            \"Remark\": r\n",
    "        })\n",
    "\n",
    "    df_lulc_mapping = pd.DataFrame(mapping_records)\n",
    "\n",
    "    # 如果之前已经生成了映射表，则追加\n",
    "    mapping_file = f\"{mode}_FeatureScore_SHAP_mapping.xlsx\"\n",
    "    if os.path.exists(mapping_file):\n",
    "        df_existing = pd.read_excel(mapping_file)\n",
    "        df_final = pd.concat([df_existing, df_lulc_mapping], ignore_index=True)\n",
    "    else:\n",
    "        df_final = df_lulc_mapping\n",
    "\n",
    "    outname = f\"{mode}_FeatureScore_SHAP_mapping_with_LULC.xlsx\"\n",
    "    df_final.to_excel(outname, index=False)\n",
    "    print(f\"  Saved {mode} mapping table with LULC to: {outname}\")\n",
    "\n",
    "print(\"\\nAll LULC SHAP mapping tables generated successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
